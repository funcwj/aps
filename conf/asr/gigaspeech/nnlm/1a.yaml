# cmd_args:
#   batch_size: 128
#   dev_batch_factor: 1
#   device_ids: '0'
#   dict: data/gigaspeech/dict
#   distributed: none
#   epochs: 120
#   eval_interval: 3000
#   init: ''
#   num_workers: 8
#   prog_interval: 250
#   resume: ''
#   save_interval: -1
#   seed: '888'
#   tensorboard: false
#   trainer: ddp

nnet: asr@rnn_lm

nnet_conf:
  dropout: 0.2
  embed_size: 1024
  hidden_size: 1024
  num_layers: 3
  rnn: lstm
  tie_weights: true

task: asr@lm

task_conf:
  bptt_mode: true

trainer_conf:
  clip_gradient: 10
  lr_scheduler_kwargs:
    factor: 0.8
    min_lr: 1.0e-08
    patience: 1
  no_impr: 15
  no_impr_thres: 0.05
  optimizer: adam
  optimizer_kwargs:
    lr: 0.001
    weight_decay: 1.0e-05
  report_metrics:
  - loss
  - accu
  - '@ppl'
  stop_criterion: '@ppl'

data_conf:
  fmt: lm@bptt
  loader:
    kaldi_format: true
    bptt_size: 128
    max_token_num: 300
    min_token_num: 5
  train:
    text: data/gigaspeech/train_xl/token
  valid:
    text: data/gigaspeech/dev/token
